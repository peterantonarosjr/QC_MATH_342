---
title: "Lab 5"
author: "Your Name Here"
output: pdf_document
---


We will work with the diamonds dataset from last lecture:

```{r}
pacman::p_load(ggplot2) #this loads the diamonds data set too
?diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)
diamonds$color =    factor(diamonds$color, ordered = FALSE)
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)
skimr::skim(diamonds)
```

Given the information above, what are the number of columns in the raw X matrix?

There are 10 columns in the raw X matrix. This can be confirmed by using ncol(MATRIX) or by viewing the summary above.

Verify this using code:

```{r}
ncol(diamonds)
```

Would it make sense to use polynomial expansions for the variables cut, color and clarity? Why or why not?

#TO-DO

Would it make sense to use log transformations for the variables cut, color and clarity? Why or why not?

#TO-DO

In order to ensure there is no time trend in the data, randomize the order of the diamond observations in D:.

```{r}
length = dim(diamonds)[1]
randObservations = sample(nrow(diamonds), length)
diamonds = diamonds[randObservations,]
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point()
```

Let's also concentrate only on diamonds with <= 2 carats to avoid the issue we saw with the maximum. So subset the dataset. Create a variable n equal to the number of remaining rows as this will be useful for later. Then plot it.

```{r}
caratSubset = diamonds[which(diamonds$carat<=2),]
n = nrow(diamonds) - nrow(caratSubset)
ggplot(caratSubset, aes(x = carat, y = price)) + 
  geom_point()

```

Create a linear model of price ~ carat and gauge its in-sample performance using s_e.

```{r}
x = caratSubset$carat
y = caratSubset$price
pricePerCarat = lm(y~x)
standardError = sd(pricePerCarat$residuals)
standardError
```

Create a model of price ~ clarity and gauge its in-sample performance

```{r}
x = caratSubset$clarity
y = caratSubset$price
priceClarity = lm(y~x)
se_0 = sd(priceClarity$residuals)
se_0
```

Why is the model price ~ carat substantially more accurate than price ~ clarity?

The model price ~ carat is substantially more accurate because its simply a more "important" feature in our phenomena. Clarity is also a categorical variable, 
which means fitting a line to this will most likely have more error than fitting to a continuous variable such as carat. (I believe its continuous, or maybe very granular discrete I can't tell nor do I know that much about diamonds)

Create a new transformed feature ln_carat and plot it vs price.

```{r}
caratSubset = transform(caratSubset, ln_carat = log(carat))
#ln_carat = log(caratSubset$carat)
ggplot(caratSubset, aes(x = ln_carat, y = price)) + 
  geom_point()
```

Would price ~ ln_carat be a better fitting model than price ~ carat? Why or why not?

It depends what sort of model we are fitting, if we are talking about a linear one then I would not use the log transformation. If we were using a second order polynomial then the log transformation would be a better option.

Verify this by comparing R^2 and RMSE of the two models:

```{r}
mod1 = lm(caratSubset$price ~ caratSubset$carat)
mod2 = lm(caratSubset$price ~ caratSubset$ln_carat)
mod1RMSE = sqrt(mean(mod1$residuals^2))
mod1R_SQ = summary(mod1)$r.squared
mod2RMSE = sqrt(mean(mod2$residuals^2))
mod2R_SQ = summary(mod2)$r.squared
mod1RMSE
mod1R_SQ
mod2RMSE
mod2R_SQ
```

Create a new transformed feature ln_price and plot its estimated density:


```{r}
caratSubset = transform(caratSubset, ln_price = log(price))
ggplot(caratSubset) + geom_histogram(aes(x = ln_price), binwidth = 0.01)
```


Now plot it vs carat.

```{r}
ggplot(caratSubset, aes(x = carat, y = ln_price)) + 
  geom_point()
```

Would ln_price ~ carat be a better fitting model than price ~ carat? Why or why not?

No it would not be a better fit model 

Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous.

```{r}
logPricePerCarat = lm(caratSubset$ln_price ~ caratSubset$carat)
se = sd(logPricePerCarat$residuals) #This is in terms of our new log scaling
exp(se) #Returns 1.397298 which indicates a +/- 39.7298% change in price when carat changes by 1
```

We just compared in-sample statistics to draw a conclusion on which model has better performance. But in-sample statistics can lie! Why is what we did valid?

What we did is valid because we are merely evaluating the fit of the line without over-fitting more features. We are comparing two models who are using the same feature against the price response and transforming error so that they are both in the same units. We are not saying who will perform better on out of sample data, but what is currently a better fit to the data.

Plot ln_price vs ln_carat.

```{r}
ggplot(caratSubset, aes(x = ln_carat, y = ln_price)) + 
  geom_point()
```

Would ln_price ~ ln_carat be the best fitting model than the previous three we considered? Why or why not?

Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous.

```{r}
logModel = lm(caratSubset$ln_price ~ caratSubset$ln_carat)
```

Compute b, the OLS slope coefficients for this new model of ln_price ~ ln_carat.

```{r}
logModel$coefficients
```

Interpret b_1, the estimated slope of ln_carat.

For every 1 unit(log units) change for carot their is ~69% change inthe price of a diamond

Interpret b_0, the estimated intercept.

With a 0 caraot diamond in log units

Create other features ln_x, ln_y, ln_z, ln_depth, ln_table.

```{r}
caratSubset = transform(caratSubset, ln_x = log(caratSubset$x))
caratSubset = transform(caratSubset, ln_y = log(caratSubset$y))
caratSubset = transform(caratSubset, ln_z = log(caratSubset$z))
caratSubset = transform(caratSubset, ln_depth = log(caratSubset$depth))
caratSubset = transform(caratSubset, ln_table = log(caratSubset$table))

```

From now on, we will be modeling ln_price (not raw price) as the prediction target. 

Create a model (B) of ln_price on ln_carat interacted with clarity and compare its performance with the model (A) ln_price ~ ln_carat.

```{r}
ggplot(caratSubset, aes(x = ln_carat, y = ln_price, color = clarity)) + geom_point()
moda = lm(ln_price ~ ln_carat, caratSubset)
modb = lm(ln_price ~ ln_carat * clarity, caratSubset)
summary(moda)$sigma
summary(modb)$sigma
```

Which model does better? Why?

B Model does better. I would say this is the result of interacting the feature ln_carat with clarity as everything else in the model remains the same. We've explained more of the variance hence the reduction in RMSE through feature interaction (differential dependence between independent features)

Create a model of (C) ln_price on ln_carat interacted with every categorical feature (clarity, cut and color) and compare its performance with model (B)

```{r}
modc = lm(ln_price ~ (ln_carat) * (clarity + cut  + color), caratSubset)
summary(modc)$sigma
summary(modb)$sigma
```

Which model does better? Why?

Our Model C does better since it is interacting between features clarity, cut and color. We are 'capturing the fit to the data' better because we are considering the differential slopes among more relevant features. Interacting more features with each other is akin to adding more features without "actually" adding new features. We are squeezing out as much as we can from these relationships.

Create a model (D) of ln_price on every continuous feature (logs of carat, x, y, z, depth, table) interacted with every categorical feature (clarity, cut and color) and compare its performance with model (C).

```{r}
caratSubset2  = caratSubset[which(caratSubset$x>0 & caratSubset$y>0 & caratSubset$z>0),]
modd = lm(ln_price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (clarity + cut  + color), caratSubset2)
summary(modd)$sigma
summary(modc)$sigma
```

Which model does better? Why?

Model D does bettor because now we are capturing the interactions between 6 features * 3 features vs 1 feature * 3 features as in Model C. 

What is the p of this model D? Compute with code.

```{r}
modd$rank
ncol(model.matrix(~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (clarity + cut  + color), caratSubset2))
```

Create model (E) which is the same as before except create include the raw features interacted with the categorical features and gauge the performance against (D).

```{r}
mod_e = lm(ln_price ~ (carat + x + y + z + depth + table) * (clarity + cut  + color), caratSubset2)
summary(mod_e)$sigma
summary(modd)$sigma
```

Which model does better? Why?

Model E does better, but the returns are diminishing at this point. We are no longer using the log transformations, but now are using the raw features. This is more accurate in the 4th decimal place, so with hesitance I would say this is a slightly more accurate representation of the data.

Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). By this time you're getting good with R's formula syntax!

```{r}
modf = lm(ln_price ~ (poly(carat,3) + poly(x,3) + poly(y,3) + poly(z,3) + poly(depth,3) + poly(table,3)) * (clarity + cut  + color), caratSubset2)
summary(modf)$sigma
summary(mod_e)$sigma
```

Which model does better? Why?

Model F does better, but at this point I am not sure at what cost. We can "play" this game forever and use higher and higher order polynomials. This is guaranteed to eventually lead to over fitting (if we aren't already) given the fact that its been proven "any continuous function can be nearly perfectly modeled by an nth order polynomial"

Can you think of any other way to expand the candidate set curlyH? Discuss.

A simple expansion of curlyH would be as we said in our previous answer to increase the order of the polynomial.
Another way would be to use different transformations that are not linearly dependent on one another. For example, using Trigonometric transformations would expand curlyH to a new space. (Probably also wouldn't fit the data well so I wouldn't do this)

We should probably assess oos performance now. Sample 2,000 diamonds and use these to create a training set of 1,800 random diamonds and a test set of 200 random diamonds. Define K and do this splitting:

```{r}
K = 10
set.seed(1984)
sampleNum = 2000
splitIndex = nrow(D)*(1-1/K)
D = caratSubset2[sample(1:nrow(caratSubset2), sampleNum), ] #Random Sample of actual full data set
Dtrain = D[1:splitIndex,] #With K=10 we are selecting the first 90% of the random sample
Dtest = D[(splitIndex+1):nrow(D),]#With K=10 we are selecting the last 10% of the random sample
```

Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 1,800! 

```{r}
inSampleSE = list()
outSampleSE = list()

moda = lm(ln_price ~ ln_carat, Dtrain)
inSampleSE[['A']] = sd(moda$residuals)
inSampleSE[['B']] = sd(modb$residuals)
inSampleSE[['C']] = sd(modc$residuals)
inSampleSE[['D']] = sd(modd$residuals)
inSampleSE[['E']] = sd(mod_e$residuals)
inSampleSE[['F']] = sd(modf$residuals)

outSampleSE[['A']] = sd(Dtest$ln_price - predict(moda,Dtest))
outSampleSE[['B']] = sd(Dtest$ln_price - predict(modb,Dtest))
outSampleSE[['C']] = sd(Dtest$ln_price - predict(modc,Dtest))
outSampleSE[['D']] = sd(Dtest$ln_price - predict(modd,Dtest))
outSampleSE[['E']] = sd(Dtest$ln_price - predict(mod_e,Dtest))
outSampleSE[['F']] = sd(Dtest$ln_price - predict(modf,Dtest))

sd(priceClarity$residuals)

unlist(inSampleSE)
unlist(outSampleSE)
```

You computed oos metrics only on n_* = 200 diamonds. What problem(s) do you expect in these oos metrics?

They're not that accurate since you are only testing on a small portion of the data. Professor Kapelner coined this "lucky" since we are not giving a true representation on testing all of the data without training on all of the data. This is where K-Fold comes in as it helps find a better average representation of how our oos metrics will do. Notice how every time we run this, a slightly different partition in D will result in different oos metrics. We want to avoid this!

To do the K-fold cross validation we need to get the splits right and crossing is hard. I've developed code for this already. Run this code.

```{r}
temp = rnorm(n)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
head(folds_vec, 200)
```

Comment on what it does and how to use it to do a K-fold CV:

#TO-DO

Do the K-fold cross validation for model F and compute the overall s_e and s_s_e. 

```{r}
kFold_se = 0
kFold_sse = 0
```

Does K-fold CV help reduce variance in the oos s_e? Discuss.

K-Fold helps reduce the variance in the dataset partitions which then gives you a more accurate representation of the oos metrics. When I say more accurate, the overall distribution of error for a particular K value is what's becoming less variate. This makes sense as rather than taking some random percentage split of the Dataset to test and validate on which can lead to instability in oos error, we are accounting for this with multiple splits all factoring into the final oos metric.

Imagine using the entire rest of the dataset besides the 2,000 training observations divvied up into slices of 200. Measure the oos error for each slice on Model F in a vector s_e_s_F and compute the s_s_e_F and also plot it.

```{r}
#TO-DO
ggplot(data.frame(s_e_s_F = s_e_s_F)) + geom_histogram(aes(x = s_e_s_F))
```

