v = 1:100
my_reverse = function(v){
vLen = length(v)
vr = array(NA,vLen)
for(i in 1:vLen){
vr[i]=v[vLen-i+1]
}
vr
}
my_reverse(v)
flip_matrix = function(X, dim_to_rev=NA){
rowDim = nrow(X)
colDim = ncol(X)
if(is.na(dim_to_rev)){
if(rowDim==colDim){
stop("bad dim_to_rev and equal rows and columns")
}
if(rowDim>colDim){
dim_to_rev = "r"
}else{
dim_to_rev = "c"
}
}
if(dim_to_rev=="r"){
X[rowDim:1,]
}else if(dim_to_rev=="c"){
X[,colDim:1]
}else{
stop("dim_to_rev needs to be 'r' or 'c'")
}
}
my_list = list()
for(i in 1:8){
my_list[[LETTERS[i]]] = array(1:(i^i),dim=rep(i,i))
}
lapply(my_list, object.size)
?object.size
#remove(my_list,i,v,flip_matrix,my_reverse)
#The following is nicer because we are "automatically" putting all objects/functions in a list for removal rather than manually specifying
rm(list = ls())
v = 1:100
my_reverse = function(v){
vLen = length(v)
vr = array(NA,vLen)
for(i in 1:vLen){
vr[i]=v[vLen-i+1]
}
vr
}
my_reverse(v)
flip_matrix = function(X, dim_to_rev=NA){
rowDim = nrow(X)
colDim = ncol(X)
if(is.na(dim_to_rev)){
if(rowDim==colDim){
stop("bad dim_to_rev and equal rows and columns")
}
if(rowDim>colDim){
dim_to_rev = "r"
}else{
dim_to_rev = "c"
}
}
if(dim_to_rev=="r"){
X[rowDim:1,]
}else if(dim_to_rev=="c"){
X[,colDim:1]
}else{
stop("dim_to_rev needs to be 'r' or 'c'")
}
}
my_list = list()
for(i in 1:8){
my_list[[LETTERS[i]]] = array(1:(i^i),dim=rep(i,i))
}
lapply(my_list, object.size)
?object.size
lorem = "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi posuere varius volutpat. Morbi faucibus ligula id massa ultricies viverra. Donec vehicula sagittis nisi non semper. Donec at tempor erat. Integer dapibus mi lectus, eu posuere arcu ultricies in. Cras suscipit id nibh lacinia elementum. Curabitur est augue, congue eget quam in, scelerisque semper magna. Aenean nulla ante, iaculis sed vehicula ac, finibus vel arcu. Mauris at sodales augue. "
splitList = strsplit(x=lorem, split = "\\.\\s")
splitLength = length(splitList[[1]])
samplingList = vector(mode = "list", length = splitLength)
for(i in 1:splitLength){
samplingList[[i]] = paste(splitList[[1]][i],". ",sep="")
}
finalList = sample(samplingList,size=splitLength,replace=FALSE)
paste(finalList, collapse = '')
my_list = list()
my_list$m = list()
my_list$f = list()
my_list$m$Millennial = strsplit("Zachary, Dylan, Christian, Wesley, Seth, Austin, Gabriel, Evan, Casey, Luis",split = ", ")[[1]]
my_list$m$GenX = strsplit("Marc, Jamie, Greg, Darryl, Tim, Dean, Jon, Chris, Troy, Jeff",split = ", ")[[1]]
my_list$m$Boomer = strsplit("Theodore, Bernard, Gene, Herbert, Ray, Tom, Lee, Alfred, Leroy, Eddie",split = ", ")[[1]]
my_list$f$GenX = strsplit("Tracy, Dawn, Tina, Tammy, Melinda, Tamara, Tracey, Colleen, Sherri, Heidi",split = ", ")[[1]]
my_list$f$Boomer = strsplit("Gloria, Joan, Dorothy, Shirley, Betty, Dianne, Kay, Marjorie, Lorraine, Mildred",split = ", ")[[1]]
my_list$f$Millennial = strsplit("Samantha, Alexis, Brittany, Lauren, Taylor, Bethany, Latoya, Candice, Brittney, Cheyenne",split = ", ")[[1]]
my_list
install.packages("pacman")
install.packages("ggplot2")
install.packages("pacman")
pacman::p_load(testthat)
v = seq(-100, 100)
expect_equal(v, -100 : 100)
v
expect_equal(my_reverse(v), rev(v))
expect_equal(my_reverse(v), rev(v))
library(datasets)
data(iris)
summary(iris)
skimr::skim(iris)
iris
cleanedIris = iris[(iris$Species!="virginica"),]
cleanedIris
y = rep(NA,nrow(cleanedIris))
binary_classify = function(input){
return(as.integer(input!="setosa"))
}
for(i in 1:length(y)){
y[i] = binary_classify(as.vector(cleanedIris$Species)[i])
}
#If unique elements all appear same # of times, let R decide what to return as the mode
mode = function(vec){
samp = unique(vec)
samp[which.max(tabulate(match(vec,samp)))]
}
mode(y)
dataLen = nrow(cleanedIris)
numOfErrors = array(NA,dataLen)
for(i in 1:dataLen){
currThreshCheck = as.numeric(cleanedIris$Sepal.Length > cleanedIris$Sepal.Length[i])
numOfErrors[i] = sum(currThreshCheck!=y)
}
theta = cleanedIris$Sepal.Length[which.min(numOfErrors)]
plot(as.vector(cleanedIris$Sepal.Length),y,xlab="Sepal Length", ylab="Binary Classification")
abline(v = theta)
#Errors per theta "threshold" value
numOfErrors
theta
summary(iris[iris$Species == "setosa", "Sepal.Length"])
summary(iris[iris$Species == "versicolor", "Sepal.Length"])
gModel = function(x){
as.numeric(x > theta)
}
sum(gModel(cleanedIris$Sepal.Length) != y)
#' TO-DO: Provide a name for this function: perceptron_learning_algorithm
#'
#' TO-DO: This function implements the perceptron algorithm; Supervised Learning method of binary classifiers. Will attempt to converge to a line which discriminates the binary responses
#'
#' @param Xinput      TO-DO: Matrix of features, can be "p" of them -> [(X1...Xn),(X2...Xn)...(Xp...Xn)]
#' @param y_binary    TO-DO: Explain this -> Vector of Binary classification (labels)
#' @param MAX_ITER    TO-DO: Explain this -> Max allowable iterations to run (in case we never achieve "low enough" error)
#' @param w           TO-DO: Explain this -> The input layers
#'
#' @return            The computed final parameter (weight) as a vector of length p + 1
perceptron_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 1000, w = NULL){
Xinput = as.matrix(cbind(1,Xinput))
p = ncol(Xinput) #Number of features
w = rep(0,p) # Initialize w
for(iteration in 1:MAX_ITER){ #Max iterations for "tuning" the line
for(row in 1:nrow(Xinput)){ #Iterating over our matrix
x_i = Xinput[row,]  #ith matrix row
y_i = y_binary[row]  #ith Binary Classification
yHat_i = ifelse(sum(x_i*w)>0,1,0)  #ith yHat
for(currFeature in 1:p){
w[currFeature] = w[currFeature] + ((y_i-yHat_i)*(x_i[currFeature]))
}
}
}
return(w)
}
Xy_simple = data.frame(
response = factor(c(0, 0, 0, 1, 1, 1)), #nominal -> Classification of "Data Points" (x1,x2)
first_feature = c(1, 1, 2, 3, 3, 4),    #continuous x1 (x1,x2) -> Data Points
second_feature = c(1, 2, 1, 3, 4, 3)    #continuous x2 (x1,x2) -> Data Points
)
pacman::p_load(ggplot2)
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) +
geom_point(size = 5)
simple_viz_obj
w_vec_simple_per = perceptron_learning_algorithm(
cbind(Xy_simple$first_feature, Xy_simple$second_feature),
as.numeric(Xy_simple$response == 1))
w_vec_simple_per
simple_perceptron_line = geom_abline(
intercept = -w_vec_simple_per[1] / w_vec_simple_per[3],
slope = -w_vec_simple_per[2] / w_vec_simple_per[3],
color = "orange")
simple_viz_obj + simple_perceptron_line
pacman::p_load(ggplot2)
Xy_simple = data.frame(
response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) +
geom_point(size = 5)
simple_viz_obj
pacman::p_load(e1071)
svm_model = svm(
formula = Xy_simple$response ~.,
data = Xy_simple,
kernel = "linear",
scale = FALSE
)
?svm
pacman::p_load(e1071)
svm_model = svm(
formula = Xy_simple$response ~.,
data = Xy_simple,
kernel = "linear",
scale = FALSE
)
w_vec_simple_svm = c(
svm_model$rho, #the b term
-t(svm_model$coefs) %*% cbind(Xy_simple$first_feature, Xy_simple$second_feature)[svm_model$index, ] # the other terms
)
simple_svm_line = geom_abline(
intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3],
slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3],
color = "purple")
simple_viz_obj + simple_svm_line
w_vec_simple_per = perceptron_learning_algorithm(
cbind(Xy_simple$first_feature, Xy_simple$second_feature),
as.numeric(Xy_simple$response == 1)
)
simple_perceptron_line = geom_abline(
intercept = -w_vec_simple_per[1] / w_vec_simple_per[3],
slope = -w_vec_simple_per[2] / w_vec_simple_per[3],
color = "orange")
simple_viz_obj + simple_perceptron_line + simple_svm_line
